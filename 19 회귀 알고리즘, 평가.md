# 19. 회귀 알고리즘, 평가

# 회귀 분석

### 종속 변수(목표 변수)와 하나 이상의 독립 변수 간의 관계를 모델링 하는 기법

## 목적

- 연속적인 값 예측
- ex)
    - 주택 가격, 온도
- 독립 변수의 중요성과 영향을 이해

## 회귀 분석의 유형

### 선형 회귀(Linear Regression)

- 독립 변수와 종속 변수 간의 선형 관계를 모델링

| 장점 | 단점 |
| --- | --- |
| 간단하고 해석 가능 | 데이터가 선형적이지 않으면 성능 저하 |
| 선형 데이터에 적합 |  |
- 공식
    - y = β₀ + β₁x + ε
- y : 예측값
- x : 독립 변수
- β₀, β₁ : 회귀 변수(절편과 기울기)
- ε : 오차(term)

### 다항 회귀(Polynomial Regression)

- 데이터의 곡선 관계를 모델링

| 장점 | 단점 |
| --- | --- |
| 곡선 데이터를 모델링 가능 | 과접합 위험 |
- 공식

![image.png](image.png)

- 다항식(polynomial)을 이용하여 곡선 관계를 모델링

### 랜덤 포레스트 회귀(Random Forest Regression)

- 앙상블 학습을 이용해 복잡한 비선형 관계를 모델링

# 성능 저하의 주요 원인

## 과적합(Overfitting)

- 모델이 학습 데이터에 지나치게 맞춰져 새로운 데이터(테스트 데이터)에서 잘 작동하지 않는 상태
- 학습 데이터의 성능은 매우 좋지만 테스트 데이터에서 성능이 떨어짐
- 너무 복잡한 모델(매우 많은 파라미터 또는 특징)을 사용한 경우 발생

## 과소적합(Underfitting)

- 모델이 학습 데이터에서 중요한 패턴을 충분히 학습하지 못해 학습 데이터와 테스트 데이터 모두에서
성능이 낮은 상태
- 학습 데이터와 테스트 데이터에서 성능이 좋지 않음
- 너무 단순한 모델을 사용한 경우에 발생

# 회귀 평가 지표

| 지표 | 설명 | 특징 | 예시 |
| --- | --- | --- | --- |
| MSE | 평균 제곱 오차 | 이상치에 민감 | 금융 데이터, 오차가 중요한 문제 |
| RMSE | MSE의 제곱근 | 실제 단위와 동일, 직관적 해석 가능 | 주택 가격, 판매량 예측 |
| MAE | 평균 절대 오차 | 이상치에 덜 민감 | 날씨 예측, 일상적인 성능 평가 |
| R² | 결정 계수 | 모델의 설명력을 나타냄 | 전반적인 회귀 모델 평가 |

## 1. MSE(MeanSquaredError)

- 평균 제곱 오차 값이 작을 수록 예측이 정화
- MSE = (1/n) * Σ(yᵢ - ŷᵢ)²
ᵢ=1

![image.png](image%201.png)

## 2. RMSE(RootMeanSquaredError)

- MSE의 제곱근으로 실제 단위와 동일
- RMSE = √(MSE)

![image.png](image%202.png)

## 3. MAE(MeanAbsoluteError)

- 절대 오차의 평균 이상치에 덜 민감
- MAE = (1/n) * Σ|yᵢ - ŷᵢ|
ᵢ=1

![image.png](image%203.png)

## 4. R²

- 모델이 데이터를 얼마나 잘 설명하는지를 나타냄

![image.png](image%204.png)

# 실습

## 1. 선형 회귀 알고리즘

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# 샘플데이터 생성
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 3 * X + np.random.randn(100, 1) * 3

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 선형 회귀 모델 학습
lr = LinearRegression()
lr.fit(X_train, y_train)

# 예측
y_pred = lr.predict(X_test)

# 평가
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R^2 Score:", r2_score(y_test, y_pred))
```

## 2. 다항 회귀 알고리즘

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# 다항 회귀 모델 생성 (차수=2)
poly_reg = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
poly_reg.fit(X_train, y_train)

# 예측 및 평가
y_pred_poly = poly_reg.predict(X_test)
print("Polynomial Regression MSE:", mean_squared_error(y_test, y_pred_poly))
print("Polynomial Regression R^2 :", r2_score(y_test, y_pred_poly))
```

## 3. 랜덤 포레스트 회귀 알고리즘

```python
from sklearn.ensemble import RandomForestRegressor

# 랜덤 포레스트 회귀 모델 학습
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train.ravel())

# 예측 및 평가
y_pred_rf = rf.predict(X_test)
print("Random Forest MSE:", mean_squared_error(y_test, y_pred_rf))
print("Random Forest R^2 :", r2_score(y_test, y_pred_rf))
```

## 4. 실습

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# 실제값과 예측값
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]

# MSE 계산
mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse:.2f}")

# RMSE 계산
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.2f}")

# MAE 계산
mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae:.2f}")

# R^2 계산
r2 = r2_score(y_true, y_pred)
print(f"R^2: {r2:.2f}")
```