# 18. 배깅, 랜덤 포레스트

# 지도학습 알고리즘

### 입력 데이터(X)와 출력 레이블(y)을 기반으로 학습하여 새로운 데이터에 대한
예측을 수행하는 알고리즘

## 분류(Classification)

- 범주형 데이터를 예측
- ex)
    - 질병 진단, 이메일 스팸 여부 등

### 알고리즘

- 로지스틱 회귀(Logistic Regression)
- 서포트 벡터 머신(SVM)
- 결정 트리(Decision Tree)
- 랜덤 포레스트(Random Forest)
- K-최근접 이웃(k-NN)
- Naive Bayes

## 회귀(Regression)

- 연속적인 값을 예측
- ex)
    - 주택 가격 예측, 주식 시장 예측

### 알고리즘

- 선형 회귀(Linear Regression)
- 다항 회귀(Polynomial Regression)
- 릿지/랏쏘 회귀(Ridge/Lasso Regression)

## 랜덤 포레스트(RandomForest)

- 여러 개의 결정 트리를 학습하고 앙상블(ensemble) 방식을 통해 최종 예측을 수행하는 알고리즘

### 작동 원리

1. 여러 결정 트리를 독립적으로 학습한다.
2. 각 트리의 예측 결과를 기반으로 다수결 투표(분류) 또는 평균값(회귀)을 계산한다.

| 장점 | 단점 |
| --- | --- |
| 과적합 방지 | 느린 예측 속도 |
| 높은 정확도 | 어려운 해석 |

## 앙상블(Ensemble)

- 머신러닝에서 여러 모델을 결합하여 예측 성능을 향상시키는 방법론을 의미

### 1. 배깅(Bagging)

- 여러 약한 학습기(결정트리)를 독립적으로 학습 시킨 후, 그 결과를 평균내거나 투표를 통해 최종 결과를 도출(랜덤 포레스트)

### 2. 부스팅(Boosting)

- 이전 학습기에서의 오차를 보완하는 방식으로 학습기를 순차적으로 학습시킴(그래디언트 부스팅, XGBoost, LightGBM, CatBoost)

### 3.스태킹(Stacking)

- 서로 다른 종류의 모델을 결합하여 최적의 조합을 찾음
- 상위 메타 모델이 하위 모델들의 출력을 학습함

## SVM(Support Vector Machine)

- 초평면(hyperplane)을 사용하여 데이터를 분리하는 분류 알고리즘

### 작동 원리

1. 데이터를 가장 잘 분리할 수 있는 최적의 초평면을 찾는다.
2. 초평면에서 가장 가까운 데이터(서포트벡터)를 기준으로 최적화를 수행한다.

| 장점 | 단점 |
| --- | --- |
| 고차원 데이터에 적합 | 대규모 데이터셋에서 느릴 수 있음 |
| 과적합 방지 효과 | 적절한 커널과 하이퍼파라미터 설정이 중요 |

## 초평면(Hyperplane)

- SVM이 사용하는 핵심 개념 중 하나로, 데이터를 분류하거나 회귀 분석을 수행할 때 사용되는 결정 경계를 의미
- 2차원에서는 초평면은 직선
- 3차원에서는 초평면은 평면
- n차원에서는 초평면은 (n-1)차원의 서브 스페이스

### 목표

- 데이터를 가장 잘 분리할 수 있는 최적의 초평면을 찾는다.
- “가장 잘 분리”란 초평면과 각 클래스의 가장 가까운 데이터 포인트(서포트 벡터)사이의 거리가 최대가 되는 것을 의미

## 커널 트릭

- SVM이 입력 데이터를 고차원 공간으로 매핑해 비선형 문제를 선형적으로 분류할 수 있게 해주는 기법

## 하이퍼 파라미터 튜닝

- 모델 학습에 영향을 미치는 사용자가 설정하는 매개 변수
- ex)
    - 결정트리의 max_depth, min_samples_split 등
- 잘못된 하이퍼 파라미터 선택은 모델 성능을 저하시킬 수 있음
- 적절한 값은 과적합과 과소적합을 방지하고 성능을 향상시킴

### 최적화 방법

- GridSearchCV
    - 모든 하이퍼 파라미터 조합을 탐색하여 최적의 조합을 선택

| 장점 | 단점 |
| --- | --- |
| 완전 탐색으로 최적 결과 보장 | 계산 비용이 높음 |
- RandomizedSearchCV
    - 랜덤 샘플링으로 하이퍼 파라미터 탐색

| 장점 | 단점 |
| --- | --- |
| 계산 비용 절감 | 최적 결과를 보장하지는 않음 |

## 실습

### 1. 아이리스 데이터셋 시각화

```python
import matplotlib.pyplot as plt
from sklearn import datasets

# Iris 데이터 로드
iris = datasets.load_iris()
X = iris.data  # 특징 데이터
y = iris.target  # 레이블 데이터
labels = iris.target_names  # 클래스 이름

# 첫 번째와 두 번째 특징(속성) 선택
X_feature_1 = X[:, 0]  # 첫 번째 특징 (sepal length)
X_feature_2 = X[:, 1]  # 두 번째 특징 (sepal width)

# 시각화
plt.figure(figsize=(8, 6))
for i, label in enumerate(labels):
    plt.scatter(X_feature_1[y == i], X_feature_2[y == i], label=label, alpha=0.7)

plt.title("Iris Dataset Visualization (Sepal Features)")
plt.xlabel("Sepal Length (cm)")
plt.ylabel("Sepal Width (cm)")
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

### 2. 배깅 - 랜덤 포레스트

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 로드
data = load_iris()
X = data.data
y = data.target

# 학습 및 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 랜덤 포레스트 모델 생성
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 예측 및 성능 평가
y_pred = rf_model.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
print(y_test)
print(y_pred)
```

### 3. 복잡한 데이터 사용

```python
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# 복잡한 데이터 생성
X, y = make_classification(
    n_samples=1000, n_features=10, n_informative=6,
    n_redundant=2, n_classes=3, random_state=42
)

# 데이터 시각화 (2개의 주요 특징만)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.6)
plt.title("Complex Dataset Example")
plt.show()
```

### 4. 모델별 차이 확인

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 분리 (이 부분은 이전 코드에서 X, y가 정의되었다고 가정합니다)
# 예시를 위해 임의의 데이터 생성
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=100, n_features=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=50, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

# Gradient Boosting
gb_model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.05, random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)

# Stacking
stack_model = StackingClassifier(
    estimators=[
        ('dt', DecisionTreeClassifier(max_depth=5, random_state=42)),
        ('svc', SVC(kernel='linear', probability=True))
    ],
    final_estimator=LogisticRegression()
)
stack_model.fit(X_train, y_train)
stack_pred = stack_model.predict(X_test)

# 결과 비교
print("Random Forest Accuracy:", accuracy_score(y_test, rf_pred))
print("Gradient Boosting Accuracy:", accuracy_score(y_test, gb_pred))
print("Stacking Accuracy:", accuracy_score(y_test, stack_pred))
```

### 5. svm

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# 데이터 로드 (Iris 데이터셋)
iris = datasets.load_iris()
X = iris.data  # 특징 데이터
y = iris.target  # 레이블 데이터

# 학습 및 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# SVM 모델 생성 (RBF 커널 사용)
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)

# 모델 학습
svm_model.fit(X_train, y_train)

# 예측
y_pred = svm_model.predict(X_test)

# 결과 출력
print("SVM Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```

### 6. svm - 복잡한 데이터

```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 복잡한 데이터 생성
X, y = make_moons(n_samples=300, noise=0.2, random_state=42)

# 데이터 시각화
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.title("Nonlinear Moon Dataset")
plt.show()

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# SVM 모델 생성 (RBF 커널 사용)
svm_model = SVC(kernel='rbf', C=1.0, gamma=0.5, random_state=42)
svm_model.fit(X_train, y_train)

# 예측
y_pred = svm_model.predict(X_test)

# 결과 출력
print("SVM Accuracy on Nonlinear Data:", accuracy_score(y_test, y_pred))
```