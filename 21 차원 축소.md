# 21. 차원 축소

# 차원 축소

### 고차원의 데이터를 더 낮은 차원으로 변환하여 데이터를 효율적으로 분석하고 시각화하는 기법

## 목적

- 데이터 처리 시간과 메모리 감소
- 모델의 과적합 방지
- 데이터 시각화 가능(2D/3D로 변환
- ex)
    - 고등학생 성적 데이터(수학, 과학, 영어, 국어 → 문과/이과)

## 장/단점

- 고차원의 데이터는 차원의 저주(Curse of Dimensionality)문제를 야기
- 차원이 증가하면 데이터의 분포가 희박해짐
- 학습 모델 성능 저하

| 장점 | 단점 |
| --- | --- |
| 데이터 시각화 가능 | 정보 손실 가능성 |
| 과적합 위험 감소 | 해석이 어려울 수 있음(PCA의 주성분은 기존의 의미가 모호) |
| 계산 속도와 메모리 효율 향상 | 알고리즘에 따라 결과가 달라질 수 있음 |

## 특성 선택(Feature Selection)

- 가장 중요한 특정만 골라 차원을 줄임

### 방법

- 상관계수 기반으로 상관성이 높은 특성 선택
- L1 정규화를 활용한 특성 선택(라쏘 회귀)

### 예시

- 고객 구매 데이터를 분석할 때, ‘나이’와 ‘월 소득’은 중요하지만 ‘전화번호’는 무의미

## 특성 추출(Feature Extraction)

- 데이터에서 중요한 정보를 뽑아내는 과정
- 기존의 특성들을 결합해 새로운 축을 생성하여 차원을 줄임
- 데이터의 패턴을 보존하면서 차원을 축소

# 차원 축소 기법

## PCA(Principal Component Analysis, 주 성분 분석)

- 데이터를 분산이 가장 큰 축(Principal Component)으로 투영하여 차원을 축소

### 원리

- 데이터의 공분산 행렬을 계산 고유값과 고유 벡터를 사용해 주 성분 축을 정의 
가장 분산이 큰 축으로 데이터를 투영

### 장점

- 데이터의 분산을 최대한 유지하며 정보 손실 최소화
- ex)
    - 고차원 유전자 데이터에서 주요한 유전 변이를 찾음

### 예시

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 예제 데이터 생성
X = [[2, 3,], [3, 4], [4, 5], [5, 6], [6, 7]]
pca = PCA(n_components = 1) # 1차원으로 축소
X_pca = pca.fit_transform(X) # PCA 적용

print("Original Data : ", X)
print("Reduced Data : ", X_pca)
print(X_pca.shape)
```

## t-SNE(t-Distributed Stochastic Neighbor Embedding)

- 고차원의 데이터를 2D 또는 3D로 변환해 시각화

### 특징

- 데이터를 저차원에서 비선형적으로 매핑 데이터 포인트 간의 관계를 보존
- ex)
    - 이미지 데이터에서 비슷한 이미지를 군집화

### 예시 1

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np

# 데이터 생성
X = np.random.rand(100, 50) # 50차원의 데이터
tsne = TSNE(n_components = 2, random_state = 42)
X_embedded = tsne.fit_transform(X)
print(tsne)
print(X_embedded)

# 시각화
plt.scatter(X_embedded[:, 0], X_embedded[:, 1])
plt.title("t-SNE Visualization")
plt.show()
```

### 예시 2

```python
from sklearn.datasets import make_moons
import numpy as np

# 비선형 데이터 생성(Moon 형태의 데이터)
X, y = make_moons(n_samples = 500, noise = 0.1, random_state = 42)

print(X)
print(y)
```

```python
from sklearn.manifold import TSNE

# t-SNE로 데이터 차원 축소
tsne = TSNE(n_components = 2, perplexity = 30, random_state = 42)
X_embedded = tsne.fit_transform(X)
print(X_embedded)
```

```python
import matplotlib.pyplot as plt

# 3 # t-SNE 결과 시각화
plt.figure(figsize=(8, 6))
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='viridis', s=30, alpha=0.7)
plt.colorbar(label="Class")
plt.title("t-SNE Visualization of Nonlinear Data (Make Moons)")
plt.xlabel("t-SNE Dimension 1")
plt.ylabel("t-SNE Dimension 2")
plt.grid(alpha=0.3)
plt.show()
```

## LDA(Linear Discriminant Analysis)

- 차원 축소와 분류를 동시에 수행
- 클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하는 지도 학습 방식
- ex)
    - 학생 데이터를 문과/이과로 분류하며 차원 축소