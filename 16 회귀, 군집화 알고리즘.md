# 16. 회귀, 군집화 알고리즘

# 피처 엔지니어링

### 머신러닝과 데이터 분석에서 중요한 단계로, 모델의 성능을 높이기 위해 데이터를
변환, 조작, 또는 확장하여 유용한 피처(특징)를 생성하는 과정을 의미
데이터는 항상 불완전하거나 충분하지 않기 때문에 
좋은 피처는 모델의 성능을 향상시킴

## 과정

### 1. 데이터의 이해

### 2. 데이터의 특성을 파악하고 부족한 부분이나 중요한 정보 확인

### 3. 새로운 피처 생성

- 기존 데이터를 바탕으로 모델링에 적합한 변수 생성

### 4. 데이터 변환

- 데이터를 분석과 모델링에 적합한 형태로 조정

## 피처 엔지니어링의 기법

### 파생 변수 생성

- 기존 데이터를 활용하여 새로운 변수를 생성

### 범주형 변수 인코딩

- 범주형 데이터를 모델이 이해할 수 있는 숫자형 데이터로 변환
    - Label Encoding : 범주형 값을 0, 1, 2로 변환
    - One-Hot Encoding : 각 범주를 이진값으로 변환

### 날짜 데이터 처리

- 날짜 데이터를 모델링에 적합한 형태로 변환
- 변환 가능한 피처
    - 연도, 월, 요일, 분기, 주말 여부

### 비선형 변환

- 데이터 분포를 변환하여 분석과 모델 성능 향상
    - 로그 변환 : 분포가 비대칭일 때 사용
    - 제곱근 변환 : 데이터의 범위를 줄임

## 실습

```python
# 라이브러리 임포트
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Titanic 데이터 로드
df = sns.load_dataset('titanic')

# 데이터 확인
print("데이터 정보:\n", df.info())
print("\n데이터 앞 5개 행:\n", df.head())

# 1. 파생 변수 생성: 가족 크기 (Family Size)
df['family_size'] = df['sibsp'] + df['parch'] + 1  # 형제/배우자 + 부모/자녀 + 자신
print("\n가족 크기 추가:\n", df[['sibsp', 'parch', 'family_size']].head())

# 2. 범주형 변수 인코딩: 성별을 숫자로 변환
df['sex_encoded'] = df['sex'].map({'male': 0, 'female': 1}) # 남성: 0, 여성: 1
print("\n성별 인코딩 결과:\n", df[['sex', 'sex_encoded']].head())

# 3. 날짜 데이터 생성 및 처리
# 'embark_date'가 없으므로 임의의 날짜 기준으로 날짜 차이 계산
start_date = pd.to_datetime('2023-01-01')
df['embark_date'] = start_date + pd.to_timedelta(df.index, unit='D')

df['weekday'] = df['embark_date'].dt.day_name() # 요일
df['is_weekend'] = df['weekday'].isin(['Saturday', 'Sunday']).astype(int) # 주말 여부
print("\n날짜 데이터 추가:\n", df[['embark_date', 'weekday', 'is_weekend']].head())

# 4. 비선형 변환: 운임(Fare) 로그 변환
df['fare_log'] = np.log1p(df['fare']) # log(1 + fare)
print("\n운임(Fare) 로그 변환 결과:\n", df[['fare', 'fare_log']].head())

# 5. One-Hot Encoding: 승선 항구(Embarked)
df = pd.get_dummies(df, columns=['embarked'], prefix='embarked')
print("\n승선 항구 One-Hot Encoding 결과:\n", df.head())

# 6. 추가 변수 생성: 1인당 운임 (Fare per Person)
df['fare_per_person'] = df['fare'] / df['family_size']
print("\n1인당 운임 계산 결과:\n", df[['fare', 'family_size', 'fare_per_person']].head())

# 데이터 시각화: 로그 변환 전후 비교
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['fare'], bins=20, kde=True)
plt.title("Fare Distribution (Before Log Transformation)")

plt.subplot(1, 2, 2)
sns.histplot(df['fare_log'], bins=20, kde=True)
plt.title("Fare Distribution (After Log Transformation)")
plt.show()

# 가족 크기와 생존 여부 시각화
sns.boxplot(data=df, x='survived', y='family_size')
plt.title("Family Size by Survival")
plt.show()
```

## 피처 선택

- 모델 성능에 기여하는 중요한 변수를 선택하고, 의미없는 변수를 제거하는 과정

## 피처 선택의 중요성

### 모델 성능 향상

- 불필요한 변수 제거로 모델의 예측 정확도 향상

### 과적합 방지

- 너무 많은 변수를 사용하면 모델이 데이터에 과도하게 적합될 가능성 증가

### 계산 효율성 증가

- 불필요한 변수를 줄이면 모델 학습과 예측 속도가 발라짐

## 피처 선택 기법

### 필터 방법

- 데이터의 통계적 특성을 기반으로 변수 선택
- 분산 기반 선택
    - 분산이 낮은 변수 제거
- 상관계수 분석
    - 상관 관계가 높은 변수 제거

### 래퍼 방법

- 변수 조합을 평가하여 최적의 피처 집합 선택
- 순방향 선택
    - 변수를 하나씩 추가하며 성능 평가
- 후방 제거
    - 모든 변수에서 시작해 중요하지 않은 변수를 제거

### 임베디드 방법

- 모델 학습 과정에서 변수 중요도를 계산하여 선택
- Lasso 회귀
    - 변수 중요도를 제로로 만들어 불필요한 변수 제거
- Random Forest
    - 변수 중요도(Feature Importance) 활용

## Lasso 회귀

- 선형 회귀에 L1 정규화를 추가한 방법
- 변수의 계수를 절대값(L1) 기준으로 페널티를 부과하여 중요하지 않은 변수의 계수를 0으로 만든다.

### 특징

- 변수 선택 가능
- 모델 단순화
- 해석 가능성 증가

## 실습

```python
# 라이브러리 임포트
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.linear_model import LassoCV
import matplotlib.pyplot as plt

# 데이터 로드
df = sns.load_dataset('titanic')
df

# 결측치 처리
df['age'].fillna(df['age'].median(), inplace=True)
df['fare'].fillna(df['fare'].median(), inplace=True)
df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)

# 수치형 변수 선택
df_num = df.select_dtypes(include=np.number)

# 1. 분산 기반 선택
low_variance_cols = df_num.var()[df_num.var() < 0.5].index
print("분산이 낮은 변수:\n", low_variance_cols)
df_selected = df_num.drop(columns=low_variance_cols)
print(df_selected.head())

# 2. 상관 계수 분석
corr_matrix = df_selected.corr()

# 상관 계수가 0.8 초과인 변수 탐지
high_corr_cols = [
    col for col in corr_matrix.columns if any(corr_matrix[col] > 0.8) and col != corr_matrix.columns[0]
]

print("\n상관 관계가 높은 변수 제거 :\n", high_corr_cols)
df_selected = df_selected.drop(columns=high_corr_cols, errors='ignore')
print(df_selected.head())

# 3. Lasso 회귀를 활용한 변수 선택
X = pd.get_dummies(df.drop(columns=['survived']), drop_first=True)
y = df['survived']

# LassoCV 모델 훈련 및 변수 선택
lasso = LassoCV(cv=5, random_state=42).fit(X, y)
selected_features = X.columns[lasso.coef_ != 0]

print("\nLasso 선택된 변수:\n", selected_features)

df

# 분산 시각화
df_var = df_num.var()
df_var.plot(kind = 'bar', color = 'skyblue', figsize = (10, 6))
plt.axhline(0.5, color = 'red', linestyle = '--', label = 'Threshold(0.5)')
plt.legend()
plt.show()

# 상관 행렬 히트맵
sns.heatmap(corr_matrix, annot = True, cmap = "coolwarm", fmt = '.2f')
plt.show()

# Lasso 회귀 선택 변수 시각화
lasso_co = lasso.coef_[lasso.coef_ != 0]
plt.bar(selected_features, lasso_co, color = 'purple')
plt.xticks(rotation = 45)
plt.show()
```